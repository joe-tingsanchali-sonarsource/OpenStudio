name: TEST - Ubuntu 22.04 Only (Optimized)

on:
  workflow_dispatch:
    inputs:
      publish_to_s3:
        description: "Force S3 publishing even when not on develop"
        required: false
        default: "false"
      skip_docker_trigger:
        description: "Skip downstream docker workflow trigger"
        required: false
        default: "false"
      skip_cdash_submission:
        description: "Skip CDash submission (useful for testing without token)"
        required: false
        default: "false"
  workflow_call:
    inputs:
      publish_to_s3:
        type: string
        required: false
        default: "false"
      skip_docker_trigger:
        type: string
        required: false
        default: "false"
      skip_cdash_submission:
        type: string
        required: false
        default: "false"

concurrency:
  group: full-build-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: read
  actions: read
  checks: write
  pull-requests: write
  packages: write
  id-token: write

env:
  BUILD_TYPE: Release
  OPENSTUDIO_SOURCE: OpenStudio
  OPENSTUDIO_BUILD: OS-build-release-v2
  PY_VERSION: "3.12.2"
  AWS_S3_BUCKET: openstudio-ci-builds
  TEST_DASHBOARD_RELATIVE: Testing/dashboard/test-dashboard.md

jobs:
  linux-x64:
    name: ${{ matrix.display_name }}
    runs-on: ${{ matrix.runner }}
    container:
      image: ${{ matrix.container_image }}
      # Added --privileged to allow swap creation if needed, though sudo usually works on host
      options: ${{ matrix.container_options }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: ubuntu-2204-x64
            display_name: Ubuntu 22.04 x64
            runner: ubuntu-22.04
            container_image: nrel/openstudio-cmake-tools:jammy-main
            container_options: "--privileged -u root -e LANG=en_US.UTF-8"
            test_suffix: Ubuntu-2204
            pip_package: true
            docker_trigger: true
            upload_globs: |
              *.deb
              *OpenStudio*x86_64.tar.gz
            cpack_generators: "DEB;TGZ"
            # LOWERED TO 2 TO PREVENT MEMORY CRASHES
            max_jobs: 2 
    defaults:
      run:
        shell: bash
    env:
      MAX_BUILD_THREADS: ${{ matrix.max_jobs }}
      CTEST_PARALLEL_LEVEL: ${{ matrix.max_jobs }}
      ENABLE_COVERAGE: false
      # TODO: ENABLE_COVERAGE is intentionally left false to avoid
      # generating/collecting coverage artifacts in CI by default.
      # If you want coverage in a run, set ENABLE_COVERAGE=true and
      # ensure the container has coverage tools (lcov/gcov or python/ruby coverage).
      # Reminder: revisit coverage configuration and CI image to add required tools.
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # --- OPTIMIZATION START: ADD SWAP ---
      # This provides a safety net for the 6.17GB memory spikes
      - name: Enable Swap Space (attempt)
        # Runs inside the container as root; attempts swap if privileged
        run: |
          set -euo pipefail
          if grep -q '/swapfile' /proc/swaps; then
            echo "::notice::Swap already active"
          else
            if (fallocate -l 4G /swapfile && chmod 600 /swapfile && mkswap /swapfile && swapon /swapfile); then
              echo "::notice::Enabled 4GB swap space (container)"
            else
              echo "::warning::Failed to enable swap (likely insufficient privilege); continuing"
            fi
          fi
          free -h || true
      # --- OPTIMIZATION END ---

      - name: Restore ccache cache
        uses: actions/cache@v4
        with:
          path: ~/.ccache
          key: ccache-${{ runner.os }}-${{ matrix.platform }}-${{ github.ref }}
          restore-keys: |
            ccache-${{ runner.os }}-${{ matrix.platform }}-

      - name: Cache Conan data
        uses: actions/cache@v4
        with:
          path: ~/.conan2
          key: conan2-${{ runner.os }}-${{ matrix.platform }}-${{ github.ref }}
          restore-keys: |
            conan2-${{ runner.os }}-${{ matrix.platform }}-

      - name: Prepare workspace
        run: |
          set -euo pipefail
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          mkdir -p "$GITHUB_WORKSPACE/${{ env.OPENSTUDIO_BUILD }}"
          if command -v ccache >/dev/null 2>&1; then
            # Lowered cache size slightly to save disk space on runner
            ccache -M 2G || true
            echo "Configured ccache max size:"; ccache -s | sed -n '1,10p'
          fi

      - name: Configure Conan remotes
        run: |
          set -euo pipefail
          conan remote add conancenter https://center.conan.io --force
          conan remote update conancenter --insecure
          conan remote add nrel-v2 https://conan.openstudio.net/artifactory/api/conan/conan-v2 --force
          conan remote update nrel-v2 --insecure
          if [ ! -f "$HOME/.conan2/profiles/default" ]; then
            conan profile detect
          fi

      - name: Conan install
        run: |
          set -euo pipefail
          conan install . \
            --output-folder="${{ env.OPENSTUDIO_BUILD }}" \
            --build=missing \
            -c tools.cmake.cmaketoolchain:generator=Ninja \
            -s compiler.cppstd=20 \
            -s build_type=${{ env.BUILD_TYPE }}

      - name: Configure with CMake
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          # Ensure Bundler is installed for Ruby tests (fixes known Ubuntu test failures)
          if command -v rvm >/dev/null 2>&1; then
            echo "Installing bundler 2.4.10 (if missing)"
            source /usr/local/rvm/scripts/rvm || true
            rvm use ruby-3.2.2 || true
            gem install bundler -v 2.4.10 --no-document || gem install bundler --no-document || true
          else
            echo "rvm not found; skipping bundler install step"
          fi

          # Respect optional ENABLE_COVERAGE environment variable to turn on coverage builds
          CMAKE_COVERAGE_FLAG=""
          if [ "${ENABLE_COVERAGE:-false}" = "true" ]; then
            CMAKE_COVERAGE_FLAG="-DENABLE_COVERAGE:BOOL=ON"
            echo "Coverage enabled for this build"
          fi
          # Added Job Pools to prevent linker from consuming all RAM
          cmake -G Ninja \
            -DCMAKE_TOOLCHAIN_FILE=conan_toolchain.cmake \
            -DCMAKE_BUILD_TYPE:STRING=${{ env.BUILD_TYPE }} \
            -DBUILD_TESTING:BOOL=ON \
            $CMAKE_COVERAGE_FLAG \
            -DCPACK_GENERATORS:STRING="${{ matrix.cpack_generators }}" \
            -DBUILD_PYTHON_BINDINGS:BOOL=ON \
            -DDISCOVER_TESTS_AFTER_BUILD:BOOL=ON \
            -DBUILD_PYTHON_PIP_PACKAGE:BOOL=${{ matrix.pip_package }} \
            -DPYTHON_VERSION:STRING=${{ env.PY_VERSION }} \
            -DCMAKE_EXPORT_COMPILE_COMMANDS:BOOL=ON \
            -DCMAKE_JOB_POOL_COMPILE:STRING=compile \
            -DCMAKE_JOB_POOL_LINK:STRING=link \
            -DCMAKE_JOB_POOLS:STRING="compile=${{ matrix.max_jobs }};link=1" \
            ..

      - name: Upload configure artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: configure-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/CMakeCache.txt
            ${{ env.OPENSTUDIO_BUILD }}/compile_commands.json

      - name: Build with Ninja
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          
          echo "::notice::Building with hard limit: ${{ matrix.max_jobs }} jobs (Swap enabled)"
          
          # Start resource monitor
          echo "timestamp PID RSS_KB COMM" > mem_samples.log
          ( while true; do 
              sleep 60; 
              stamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ"); 
              if command -v ps >/dev/null 2>&1; then ps -eo pid,rsz,comm --sort=-rsz | head -n 5 | awk -v s="$stamp" '{print s" "$1" "$2" "$3}' >> mem_samples.log; fi; 
            done ) &
          HB_PID=$!
          
          # Run Build
          cmake --build . --parallel ${{ matrix.max_jobs }} 2>&1 | tee build.log
          BUILD_EXIT=${PIPESTATUS[0]}
          
          kill $HB_PID || true
          exit $BUILD_EXIT

      - name: Summarize peak memory usage
        if: always()
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          if [ -f mem_samples.log ]; then
            echo "::group::Peak Memory Summary"
            peak_cc1=$(grep -E 'cc1plus$' mem_samples.log | awk '{print $3}' | sort -nr | head -n1)
            if [ -n "$peak_cc1" ]; then
              awk -v v="$peak_cc1" 'BEGIN{printf "Peak cc1plus RSS: %.2f GB\n", v/1024/1024}'
            else
              echo "No cc1plus samples recorded"
            fi
            echo "::endgroup::"
          fi

      - name: Deferred pytest discovery (second configure)
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          cmake -G Ninja \
            -DCMAKE_TOOLCHAIN_FILE=conan_toolchain.cmake \
            -DCMAKE_BUILD_TYPE:STRING=${{ env.BUILD_TYPE }} \
            -DBUILD_TESTING:BOOL=ON \
            -DCPACK_GENERATORS:STRING="${{ matrix.cpack_generators }}" \
            -DBUILD_PYTHON_BINDINGS:BOOL=ON \
            -DDISCOVER_TESTS_AFTER_BUILD:BOOL=ON \
            -DAPPEND_TESTS_ONLY:BOOL=ON \
            -DBUILD_PYTHON_PIP_PACKAGE:BOOL=${{ matrix.pip_package }} \
            -DPYTHON_VERSION:STRING=${{ env.PY_VERSION }} \
            -DCMAKE_EXPORT_COMPILE_COMMANDS:BOOL=ON \
            -DCMAKE_JOB_POOL_COMPILE:STRING=compile \
            -DCMAKE_JOB_POOL_LINK:STRING=link \
            -DCMAKE_JOB_POOLS:STRING="compile=${{ matrix.max_jobs }};link=1" \
            ..

      - name: Run CTest suite and submit to CDash
        id: ctest
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        continue-on-error: true
        run: |
          set -euo pipefail
          . ./conanbuild.sh

          # Set build name and site for CDash dashboard
          export CTEST_BUILD_NAME="GitHub-${{ matrix.platform }}-${{ github.ref_name }}"
          export CTEST_SITE="${{ runner.name }}"

          # Run the tests first and capture the CTest exit code so we don't conflate
          # test failures with CDash submission errors. We intentionally do not
          # `set -e` on the ctest command so we can capture its exit code.
          echo "Running ctest..."
          ctest --output-on-failure -j ${{ matrix.max_jobs }} || true
          CTEST_EXIT_CODE=$?
          echo "exit_code=${CTEST_EXIT_CODE}" >> $GITHUB_OUTPUT

          # Submit to CDash using Experimental dashboard mode. Submission failures
          # (for example due to invalid tokens) should not cause the job to be
          # considered a test failure. We'll attempt submission and log warnings
          # on failure but preserve the original CTest exit code.
          echo "Submitting results to CDash (best-effort)..."
          SUBMIT_OUTPUT_FILE="cdash_submit.out"
          SUBMIT_ERROR=0

          # Allow skipping CDash submission via workflow input, and only attempt
          # submission when a CDASH token is available. This avoids unauthenticated
          # submissions that produce "Invalid Token" errors.
          SKIP_CDASH_SUBMISSION="${{ inputs.skip_cdash_submission || github.event.inputs.skip_cdash_submission || 'false' }}"
          if [ "$SKIP_CDASH_SUBMISSION" = "true" ]; then
            echo "Skipping CDash submission (skip_cdash_submission=true)"
            SUBMIT_ERROR=0
          elif [ -n "${{ secrets.CDASH_TOKEN || '' }}" ]; then
            echo "Using CDASH token to submit results"
            export CDASH_SUBMIT_TOKEN="${{ secrets.CDASH_TOKEN }}"
            ctest -D Experimental -DCDASH_SUBMIT_TOKEN=$CDASH_SUBMIT_TOKEN --output-on-failure -j ${{ matrix.max_jobs }} > "$SUBMIT_OUTPUT_FILE" 2>&1 || SUBMIT_ERROR=$?
          else
            echo "::warning::No CDASH token provided; skipping CDash submission to avoid failures"
            SUBMIT_ERROR=0
          fi

          if [ $SUBMIT_ERROR -ne 0 ]; then
            echo "::warning::CDash submission failed (exit ${SUBMIT_ERROR}). See output below:"
            sed -n '1,200p' "$SUBMIT_OUTPUT_FILE" || true

            # Try to detect common non-fatal errors like an invalid token and
            # explicitly warn rather than fail the job. Preserve CTest exit code.
            if grep -qi "invalid token" "$SUBMIT_OUTPUT_FILE" || grep -qi "Invalid Token" "$SUBMIT_OUTPUT_FILE"; then
              echo "::warning::CDash submission failed due to Invalid Token. This is non-fatal to tests."
            else
              echo "::warning::CDash submission returned non-zero exit code ${SUBMIT_ERROR}."
            fi
          else
            echo "::notice::CDash submission completed successfully"
          fi

          # Restore the original CTest exit code so downstream steps can fail
          # if tests actually failed.
          if [ ${CTEST_EXIT_CODE:-0} -ne 0 ]; then
            echo "::warning::CTest suite failed with exit code ${CTEST_EXIT_CODE}"
            exit ${CTEST_EXIT_CODE}
          fi


      - name: Create packages
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          # Explicitly only run the generators we asked for in matrix
          cpack -G "${{ matrix.cpack_generators }}" -B .

      - name: Collect and upload coverage artifacts
        if: env.ENABLE_COVERAGE == 'true'
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          echo "Collecting coverage artifacts"
          mkdir -p coverage_artifacts
          # Common coverage files
          if [ -f coverage.info ]; then cp coverage.info coverage_artifacts/ || true; fi
          if [ -f python_coverage.lcov ]; then cp python_coverage.lcov coverage_artifacts/ || true; fi
          # Ruby coverage outputs under src/cli/test
          find .. -type f \( -name "*.lcov" -o -name "python_coverage.xml" -o -name "python_coverage.lcov" -o -name "coverage.json" \) -maxdepth 5 -print | xargs -I{} cp --parents {} coverage_artifacts/ || true
          tar -czf coverage_artifacts.tar.gz coverage_artifacts || true
        continue-on-error: true

      - name: Upload coverage artifacts
        if: env.ENABLE_COVERAGE == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/coverage_artifacts.tar.gz
          
      - name: Copy Testing tree with suffix
        if: always()
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          cp -r Testing "Testing-${{ matrix.test_suffix }}"

      - name: Generate test summary
        if: always()
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          
          # Generate a simple markdown summary from CTest results
          mkdir -p "$(dirname '${{ env.TEST_DASHBOARD_RELATIVE }}')"
          
          echo "# OpenStudio Test Results - ${{ matrix.test_suffix }}" > "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "**Build:** \`${{ github.sha }}\`" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "**Branch:** \`${{ github.ref_name }}\`" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "**Platform:** ${{ matrix.display_name }}" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "**Date:** $(date -u)" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "## ðŸ“Š CDash Dashboard" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "Full test results are available on CDash:" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "**[View on CDash â†’](https://my.cdash.org/index.php?project=OpenStudio)**" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          echo "" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          
          if [ -f Testing/Temporary/LastTest.log ]; then
            echo "## Test Log (Last 50 lines)" >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
            echo '```' >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
            tail -50 Testing/Temporary/LastTest.log >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
            echo '```' >> "${{ env.TEST_DASHBOARD_RELATIVE }}"
          fi
        continue-on-error: true


      - name: Upload Testing artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: Testing-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/Testing-${{ matrix.test_suffix }}/
            ${{ env.OPENSTUDIO_BUILD }}/${{ env.TEST_DASHBOARD_RELATIVE }}

      - name: Upload build outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: packages-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/*.deb
            ${{ env.OPENSTUDIO_BUILD }}/*.rpm
            ${{ env.OPENSTUDIO_BUILD }}/*.tar.gz
            ${{ env.OPENSTUDIO_BUILD }}/*.whl

      - name: Configure AWS credentials
        if: ${{ matrix.upload_globs != '' && (github.ref == 'refs/heads/develop' || inputs.publish_to_s3 == 'true' || github.event.inputs.publish_to_s3 == 'true') }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: Publish installers to S3
        if: ${{ matrix.upload_globs != '' && (github.ref == 'refs/heads/develop' || inputs.publish_to_s3 == 'true' || github.event.inputs.publish_to_s3 == 'true') }}
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        env:
          S3_PREFIX: ${{ github.ref_type == 'tag' && format('releases/{0}', github.ref_name) || format('{0}', github.ref_name) }}
        run: |
          set -euo pipefail
          echo "Uploading artifacts to s3://${AWS_S3_BUCKET}/${S3_PREFIX}" > /dev/stderr
          while IFS= read -r pattern; do
            [ -z "$pattern" ] && continue
            for file in $(find . -maxdepth 1 -type f -name "$pattern" -print); do
              key="${S3_PREFIX}/$(basename "$file")"
              if aws s3api head-object --bucket "$AWS_S3_BUCKET" --key "$key" 2>/dev/null; then
                echo "Skipping existing ${key}" > /dev/stderr
                continue
              fi
              aws s3 cp "$file" "s3://${AWS_S3_BUCKET}/${key}" --acl public-read
              if command -v md5sum >/dev/null 2>&1; then
                md5sum "$file"
              fi
            done
          done <<'EOF'
          ${{ matrix.upload_globs }}
          EOF

      - name: Trigger docker workflow update
        if: ${{ matrix.docker_trigger && steps.ctest.outputs.exit_code == '0' && github.ref == 'refs/heads/develop' && (inputs.skip_docker_trigger != 'true') && (github.event.inputs.skip_docker_trigger != 'true') }}
        env:
          GH_TOKEN: ${{ secrets.GH_DOCKER_TRIGGER_TOKEN || secrets.GITHUB_TOKEN }}
          REF_NAME: ${{ github.ref_name }}
          REF_TYPE: ${{ github.ref_type }}
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          gh workflow run docker-build.yml \
            --ref "$REF_NAME" \
            -f ref_name="$REF_NAME" \
            -f ref_type="$REF_TYPE"

      - name: Fail job on test failures
        if: ${{ steps.ctest.outputs.exit_code != '0' }}
        run: |
          echo "::error::CTest suite failed with exit code ${{ steps.ctest.outputs.exit_code }}"
          exit 1
