name: TEST - Ubuntu 22.04 Only (GitHub-Hosted)

on:
  workflow_dispatch:
    inputs:
      publish_to_s3:
        description: "Force S3 publishing even when not on develop"
        required: false
        default: "false"
      skip_docker_trigger:
        description: "Skip downstream docker workflow trigger"
        required: false
        default: "false"
  workflow_call:
    inputs:
      publish_to_s3:
        type: string
        required: false
        default: "false"
      skip_docker_trigger:
        type: string
        required: false
        default: "false"

concurrency:
  group: full-build-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: read
  actions: read
  checks: write
  pull-requests: write
  packages: write
  id-token: write

env:
  BUILD_TYPE: Release
  OPENSTUDIO_SOURCE: OpenStudio
  OPENSTUDIO_BUILD: OS-build-release-v2
  PY_VERSION: "3.12.2"
  AWS_S3_BUCKET: openstudio-ci-builds
  TEST_DASHBOARD_RELATIVE: Testing/dashboard/test-dashboard.md

jobs:
  linux-x64:
    name: ${{ matrix.display_name }}
    runs-on: ${{ matrix.runner }}
    container:
      image: ${{ matrix.container_image }}
      options: ${{ matrix.container_options }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform: ubuntu-2204-x64
            display_name: Ubuntu 22.04 x64
            runner: ubuntu-22.04
            container_image: nrel/openstudio-cmake-tools:jammy
            container_options: "-u root -e LANG=en_US.UTF-8"
            test_suffix: Ubuntu-2204
            pip_package: true
            docker_trigger: true
            upload_globs: |
              *.deb
              *OpenStudio*x86_64.tar.gz
            cpack_generators: "DEB;TGZ"
            max_jobs: 4
          # Ubuntu 24.04, macOS, and Windows jobs removed for initial focused testing
    defaults:
      run:
        shell: bash
    env:
      MAX_BUILD_THREADS: ${{ matrix.max_jobs }}
      CTEST_PARALLEL_LEVEL: ${{ matrix.max_jobs }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # Restore ccache to speed up repeated builds (cache lives in container HOME)
      - name: Restore ccache cache
        uses: actions/cache@v4
        with:
          path: ~/.ccache
          key: ccache-${{ runner.os }}-${{ matrix.platform }}-${{ github.ref }}
          restore-keys: |
            ccache-${{ runner.os }}-${{ matrix.platform }}-

      - name: Cache Conan data
        uses: actions/cache@v4
        with:
          path: ~/.conan2
          key: conan2-${{ runner.os }}-${{ matrix.platform }}-${{ github.ref }}
          restore-keys: |
            conan2-${{ runner.os }}-${{ matrix.platform }}-

      - name: Prepare workspace
        run: |
          set -euo pipefail
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          mkdir -p "$GITHUB_WORKSPACE/${{ env.OPENSTUDIO_BUILD }}"
          # Configure ccache size (best-effort)
          if command -v ccache >/dev/null 2>&1; then
            ccache -M 5G || true
            echo "Configured ccache max size:"; ccache -s | sed -n '1,10p'
          fi

      - name: Configure Conan remotes
        run: |
          set -euo pipefail
          conan remote add conancenter https://center.conan.io --force
          conan remote update conancenter --insecure
          conan remote add nrel-v2 https://conan.openstudio.net/artifactory/api/conan/conan-v2 --force
          conan remote update nrel-v2 --insecure
          if [ ! -f "$HOME/.conan2/profiles/default" ]; then
            conan profile detect
          fi

      - name: Conan install
        run: |
          set -euo pipefail
          conan install . \
            --output-folder="${{ env.OPENSTUDIO_BUILD }}" \
            --build=missing \
            -c tools.cmake.cmaketoolchain:generator=Ninja \
            -s compiler.cppstd=20 \
            -s build_type=${{ env.BUILD_TYPE }}

      - name: Configure with CMake
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          cmake -G Ninja \
            -DCMAKE_TOOLCHAIN_FILE=conan_toolchain.cmake \
            -DCMAKE_BUILD_TYPE:STRING=${{ env.BUILD_TYPE }} \
            -DBUILD_TESTING:BOOL=ON \
            -DCPACK_GENERATORS:STRING="${{ matrix.cpack_generators }}" \
            -DBUILD_PYTHON_BINDINGS:BOOL=ON \
            -DDISCOVER_TESTS_AFTER_BUILD:BOOL=ON \
            -DBUILD_PYTHON_PIP_PACKAGE:BOOL=${{ matrix.pip_package }} \
            -DPYTHON_VERSION:STRING=${{ env.PY_VERSION }} \
            -DCMAKE_EXPORT_COMPILE_COMMANDS:BOOL=ON \
            ..

      - name: Upload configure artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: configure-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/CMakeCache.txt
            ${{ env.OPENSTUDIO_BUILD }}/compile_commands.json

      - name: Build with Ninja
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          export NINJA_STATUS="[%f/%t | %es elapsed | %o objs/sec]"
          # Determine dynamic parallelism based on available memory (approx 4GB per heavy compile)
          avail_kb=$(grep MemAvailable /proc/meminfo | awk '{print $2}') || avail_kb=0
          avail_gb=$((avail_kb / 1024 / 1024))
          max_jobs=${{ matrix.max_jobs }}
          dynamic_jobs=$max_jobs
          if [ $avail_gb -gt 0 ]; then
            est=$((avail_gb / 4))
            [ $est -lt 1 ] && est=1
            if [ $est -lt $dynamic_jobs ]; then
              dynamic_jobs=$est
            fi
          fi
          # If near disk full, further constrain (<=92% usage triggers halving)
          disk_used_pct=$(df -h . | awk 'NR==2{gsub(/%/,"",$5); print $5}') || disk_used_pct=0
          if [ "$disk_used_pct" -ge 92 ] && [ $dynamic_jobs -gt 1 ]; then
            dynamic_jobs=$((dynamic_jobs / 2))
            [ $dynamic_jobs -lt 1 ] && dynamic_jobs=1
          fi
          # Surface cgroup memory limit if present (cgroup v2)
          if [ -f /sys/fs/cgroup/memory.max ]; then
            cg_limit=$(cat /sys/fs/cgroup/memory.max)
            [ "$cg_limit" = "max" ] || echo "::notice::Cgroup memory.max bytes: $cg_limit"
          elif [ -f /sys/fs/cgroup/memory/memory.limit_in_bytes ]; then
            cg_limit=$(cat /sys/fs/cgroup/memory/memory.limit_in_bytes)
            echo "::notice::Cgroup v1 memory.limit_in_bytes: $cg_limit"
          fi
          echo "::notice::Adaptive parallel jobs: $dynamic_jobs (Avail=${avail_gb}GB, Max=$max_jobs, DiskUsed=${disk_used_pct}%)"
          echo "timestamp PID RSS_KB COMM" > mem_samples.log
          # Heartbeat + resource diagnostics every 5 minutes
          ( while true; do 
              sleep 300; 
              stamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ"); 
              echo "[heartbeat] $(date -u +"%H:%M:%S")"; 
              if command -v free >/dev/null 2>&1; then free -h | awk 'NR==2{print "[mem] used=" $3 "/" $2}'; fi; 
              df -h . | tail -1 | awk '{print "[disk] used=" $3 "/" $2 " (" $5 ")"}'; 
              if command -v ps >/dev/null 2>&1; then ps -eo pid,rsz,comm --sort=-rsz | head -n 12 | awk -v s="$stamp" '{print s" "$1" "$2" "$3}' >> mem_samples.log; fi; 
              ps -eo pid,pmem,rsz,comm --sort=-pmem | head -n 5 | awk '{print "[topmem] PID=" $1 " MEM%=" $2 " RSS=" $3 " " $4}';
            done ) &
          HB_PID=$!
          # Capture full build log; ensure failure propagates through pipe
          cmake --build . --parallel $dynamic_jobs 2>&1 | tee build.log
          BUILD_EXIT=${PIPESTATUS[0]}
          kill $HB_PID || true
          # Show basic ninja stats (non-fatal if unavailable)
          command -v ninja >/dev/null 2>&1 && ninja -d stats || true
          exit $BUILD_EXIT

      - name: Summarize peak memory usage
        if: always()
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          if [ -f mem_samples.log ]; then
            echo "::group::Peak Memory Summary"
            peak_cc1=$(grep -E 'cc1plus$' mem_samples.log | awk '{print $3}' | sort -nr | head -n1)
            if [ -n "$peak_cc1" ]; then
              awk -v v="$peak_cc1" 'BEGIN{printf "Peak cc1plus RSS: %.2f GB\n", v/1024/1024}'
            else
              echo "No cc1plus samples recorded"
            fi
            echo "Top 5 peak processes (by RSS KB):"
            awk 'NR>1 {proc[$4]=$3>proc[$4]?$3:proc[$4]}' mem_samples.log; awk 'NR>1 {proc[$4]=$3>proc[$4]?$3:proc[$4]} END {for (p in proc) printf "%s %.2f GB\n", p, proc[p]/1024/1024}' mem_samples.log | sort -k2 -nr | head -n5 || true
            echo "Sample lines (last 8):"; tail -n 8 mem_samples.log || true
            echo "::endgroup::"
          else
            echo "::warning::mem_samples.log missing"
          fi

      - name: Summarize build warnings
        if: always()
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          if [ -f build.log ]; then
            count=$(grep -i 'warning:' build.log | wc -l || true)
            echo "::notice::Total compiler warnings: ${count}"
            echo "Top 20 unique warning lines (normalized):"
            grep -i 'warning:' build.log | sed 's/^.*warning:/warning:/' | sort | uniq -c | sort -nr | head -n 20 || true
          else
            echo "::warning::build.log not found for warning summary"
          fi

      - name: Deferred pytest discovery (second configure)
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          cmake -G Ninja \
            -DCMAKE_TOOLCHAIN_FILE=conan_toolchain.cmake \
            -DCMAKE_BUILD_TYPE:STRING=${{ env.BUILD_TYPE }} \
            -DBUILD_TESTING:BOOL=ON \
            -DCPACK_GENERATORS:STRING="${{ matrix.cpack_generators }}" \
            -DBUILD_PYTHON_BINDINGS:BOOL=ON \
            -DDISCOVER_TESTS_AFTER_BUILD:BOOL=ON \
            -DAPPEND_TESTS_ONLY:BOOL=ON \
            -DBUILD_PYTHON_PIP_PACKAGE:BOOL=${{ matrix.pip_package }} \
            -DPYTHON_VERSION:STRING=${{ env.PY_VERSION }} \
            -DCMAKE_EXPORT_COMPILE_COMMANDS:BOOL=ON \
            ..

      - name: Upload build log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: build-log-${{ matrix.platform }}-${{ github.sha }}
          path: ${{ env.OPENSTUDIO_BUILD }}/build.log
      - name: Upload triage artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: triage-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/.ninja_log
            ${{ env.OPENSTUDIO_BUILD }}/CTestTestfile.cmake

      - name: Run CTest suite
        id: ctest
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        continue-on-error: true
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          # Adaptive test parallelism (tests may consume less memory; use 2GB heuristic)
          avail_kb=$(grep MemAvailable /proc/meminfo | awk '{print $2}') || avail_kb=0
          avail_gb=$((avail_kb / 1024 / 1024))
          max_jobs=${{ matrix.max_jobs }}
          dynamic_ctest_jobs=$max_jobs
          if [ $avail_gb -gt 0 ]; then
            est=$((avail_gb / 2))
            [ $est -lt 1 ] && est=1
            if [ $est -lt $dynamic_ctest_jobs ]; then
              dynamic_ctest_jobs=$est
            fi
          fi
          echo "::notice::Adaptive CTest jobs: $dynamic_ctest_jobs (Avail=${avail_gb}GB, Max=$max_jobs)"
          echo "exit_code=0" >> $GITHUB_OUTPUT
          ctest --output-on-failure --parallel $dynamic_ctest_jobs || {
            exit_code=$?
            echo "exit_code=${exit_code}" >> $GITHUB_OUTPUT
            echo "::warning::CTest suite failed with exit code ${exit_code}"
          }

      - name: Create packages
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          . ./conanbuild.sh
          cpack -B .

      - name: Copy Testing tree with suffix
        if: always()
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          cp -r Testing "Testing-${{ matrix.test_suffix }}"

      - name: Generate test dashboard
        if: always()
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          ruby ../developer/ruby/DashboardFromCDash.rb "${{ env.TEST_DASHBOARD_RELATIVE }}"

      - name: Upload Testing artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: Testing-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/Testing-${{ matrix.test_suffix }}/
            ${{ env.OPENSTUDIO_BUILD }}/${{ env.TEST_DASHBOARD_RELATIVE }}

      - name: Upload build outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: packages-${{ matrix.platform }}-${{ github.sha }}
          path: |
            ${{ env.OPENSTUDIO_BUILD }}/*.deb
            ${{ env.OPENSTUDIO_BUILD }}/*.rpm
            ${{ env.OPENSTUDIO_BUILD }}/*.tar.gz
            ${{ env.OPENSTUDIO_BUILD }}/*.whl

      - name: Configure AWS credentials
        if: ${{ matrix.upload_globs != '' && (github.ref == 'refs/heads/develop' || inputs.publish_to_s3 == 'true' || github.event.inputs.publish_to_s3 == 'true') }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: Publish installers to S3
        if: ${{ matrix.upload_globs != '' && (github.ref == 'refs/heads/develop' || inputs.publish_to_s3 == 'true' || github.event.inputs.publish_to_s3 == 'true') }}
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        env:
          S3_PREFIX: ${{ github.ref_type == 'tag' && format('releases/{0}', github.ref_name) || format('{0}', github.ref_name) }}
        run: |
          set -euo pipefail
          echo "Uploading artifacts to s3://${AWS_S3_BUCKET}/${S3_PREFIX}" > /dev/stderr
          while IFS= read -r pattern; do
            [ -z "$pattern" ] && continue
            for file in $(find . -maxdepth 1 -type f -name "$pattern" -print); do
              key="${S3_PREFIX}/$(basename "$file")"
              if aws s3api head-object --bucket "$AWS_S3_BUCKET" --key "$key" 2>/dev/null; then
                echo "Skipping existing ${key}" > /dev/stderr
                continue
              fi
              aws s3 cp "$file" "s3://${AWS_S3_BUCKET}/${key}" --acl public-read
              if command -v md5sum >/dev/null 2>&1; then
                md5sum "$file"
              fi
            done
          done <<'EOF'
          ${{ matrix.upload_globs }}
          EOF

      - name: Trigger docker workflow update
        if: ${{ matrix.docker_trigger && steps.ctest.outputs.exit_code == '0' && github.ref == 'refs/heads/develop' && (inputs.skip_docker_trigger != 'true') && (github.event.inputs.skip_docker_trigger != 'true') }}
        env:
          GH_TOKEN: ${{ secrets.GH_DOCKER_TRIGGER_TOKEN || secrets.GITHUB_TOKEN }}
          REF_NAME: ${{ github.ref_name }}
          REF_TYPE: ${{ github.ref_type }}
        working-directory: ${{ env.OPENSTUDIO_BUILD }}
        run: |
          set -euo pipefail
          gh workflow run docker-build.yml \
            --ref "$REF_NAME" \
            -f ref_name="$REF_NAME" \
            -f ref_type="$REF_TYPE"

      - name: Fail job on test failures
        if: ${{ steps.ctest.outputs.exit_code != '0' }}
        run: |
          echo "::error::CTest suite failed with exit code ${{ steps.ctest.outputs.exit_code }}"
          exit 1
